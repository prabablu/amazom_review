{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\",nrows=8000)\n",
    "data =  df.copy()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to save objects for later use and retireve it\n",
    "import pickle\n",
    "def savetofile(obj,filename):\n",
    "    pickle.dump(obj,open(filename+\".p\",\"wb\"))\n",
    "def openfromfile(filename):\n",
    "    temp = pickle.load(open(filename+\".p\",\"rb\"))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    7972\n",
       "True       28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Score'] = data['Score'].replace('positive',1)\n",
    "data['Score'] = data['Score'].replace('negative',0)\n",
    "data.duplicated(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>B001M09ATY</td>\n",
       "      <td>A2CW9XSV8YTGKK</td>\n",
       "      <td>Brian J. Binder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1316044800</td>\n",
       "      <td>Fanastic</td>\n",
       "      <td>Can't find this product anywhere locally - tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>B001M09ATY</td>\n",
       "      <td>AAFT7UQI6TOGE</td>\n",
       "      <td>J. Diaz \"wanna be a balabusta\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1300406400</td>\n",
       "      <td>Oatmeal makes for winter warmth</td>\n",
       "      <td>My kids have eaten oatmeal everyday since the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>B001M09ATY</td>\n",
       "      <td>A13WBUX01Q4D35</td>\n",
       "      <td>A. Beckstedt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1296950400</td>\n",
       "      <td>An oatmeal explosion of goodness</td>\n",
       "      <td>Are you sick of the regular flavors? Plain, ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>B001M09ATY</td>\n",
       "      <td>A22097Y3ZY7M14</td>\n",
       "      <td>Don D.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1249948800</td>\n",
       "      <td>Quaker Instant Oatmeal Bakery Favorites</td>\n",
       "      <td>This particular variety box contains 3 of my v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>B001M09ATY</td>\n",
       "      <td>A35R32TA60XD57</td>\n",
       "      <td>M. Torma</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1335484800</td>\n",
       "      <td>Sorry excuse for oatmeal</td>\n",
       "      <td>This sounded so tasty from the reviews and the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id   ProductId          UserId                      ProfileName  \\\n",
       "0        1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1        2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2        3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3        4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4        5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...    ...         ...             ...                              ...   \n",
       "7995  7996  B001M09ATY  A2CW9XSV8YTGKK                  Brian J. Binder   \n",
       "7996  7997  B001M09ATY   AAFT7UQI6TOGE   J. Diaz \"wanna be a balabusta\"   \n",
       "7997  7998  B001M09ATY  A13WBUX01Q4D35                     A. Beckstedt   \n",
       "7998  7999  B001M09ATY  A22097Y3ZY7M14                           Don D.   \n",
       "7999  8000  B001M09ATY  A35R32TA60XD57                         M. Torma   \n",
       "\n",
       "      HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                        1                       1      5  1303862400   \n",
       "1                        0                       0      1  1346976000   \n",
       "2                        1                       1      4  1219017600   \n",
       "3                        3                       3      2  1307923200   \n",
       "4                        0                       0      5  1350777600   \n",
       "...                    ...                     ...    ...         ...   \n",
       "7995                     0                       0      5  1316044800   \n",
       "7996                     0                       0      4  1300406400   \n",
       "7997                     0                       0      5  1296950400   \n",
       "7998                     0                       0      5  1249948800   \n",
       "7999                     0                       2      1  1335484800   \n",
       "\n",
       "                                      Summary  \\\n",
       "0                       Good Quality Dog Food   \n",
       "1                           Not as Advertised   \n",
       "2                       \"Delight\" says it all   \n",
       "3                              Cough Medicine   \n",
       "4                                 Great taffy   \n",
       "...                                       ...   \n",
       "7995                                 Fanastic   \n",
       "7996          Oatmeal makes for winter warmth   \n",
       "7997         An oatmeal explosion of goodness   \n",
       "7998  Quaker Instant Oatmeal Bakery Favorites   \n",
       "7999                 Sorry excuse for oatmeal   \n",
       "\n",
       "                                                   Text  \n",
       "0     I have bought several of the Vitality canned d...  \n",
       "1     Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2     This is a confection that has been around a fe...  \n",
       "3     If you are looking for the secret ingredient i...  \n",
       "4     Great taffy at a great price.  There was a wid...  \n",
       "...                                                 ...  \n",
       "7995  Can't find this product anywhere locally - tha...  \n",
       "7996  My kids have eaten oatmeal everyday since the ...  \n",
       "7997  Are you sick of the regular flavors? Plain, ap...  \n",
       "7998  This particular variety box contains 3 of my v...  \n",
       "7999  This sounded so tasty from the reviews and the...  \n",
       "\n",
       "[8000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = data[data.HelpfulnessNumerator <= data.HelpfulnessDenominator]\n",
    "final\n",
    "#print(\"Size of data\",final['Id'].size,\" rows \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma', 'shan', \"shan't\"]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english') #All the stopwords in English language\n",
    "#excluding some useful words from stop words list \n",
    "excluding = ['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "stop = [words for words in stop if words not in excluding]\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile('<.*?>')#Find this kind of pattern\n",
    "#     print(p.findall(data))#List of strings which follow the regex pattern\n",
    "    return p.sub('',data) #Substitute nothing at the place of strings which matched the patterns\n",
    "\n",
    "\n",
    "def strippunc(data):\n",
    "    p = re.compile(r'[?|!|\\'|\"|#|.|,|)|(|\\|/|~|%|*]')\n",
    "    return p.sub('',data)\n",
    "strippunc(\"fsd*?~,,,( sdfsdfdsvv)#\")\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer('english') #initialising the snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "['This', 'is', 'a', 'confection', 'that', 'has', 'been', 'around', 'a', 'few', 'centuries', 'It', 'is', 'a', 'light', 'pillowy', 'citrus', 'gelatin', 'with', 'nuts', '-', 'in', 'this', 'case', 'Filberts', 'And', 'it', 'is', 'cut', 'into', 'tiny', 'squares', 'and', 'then', 'liberally', 'coated', 'with', 'powdered', 'sugar', 'And', 'it', 'is', 'a', 'tiny', 'mouthful', 'of', 'heaven', 'Not', 'too', 'chewy', 'and', 'very', 'flavorful', 'I', 'highly', 'recommend', 'this', 'yummy', 'treat', 'If', 'you', 'are', 'familiar', 'with', 'the', 'story', 'of', 'CS', 'Lewis', 'The', 'Lion', 'The', 'Witch', 'and', 'The', 'Wardrobe', '-', 'this', 'is', 'the', 'treat', 'that', 'seduces', 'Edmund', 'into', 'selling', 'out', 'his', 'Brother', 'and', 'Sisters', 'to', 'the', 'Witch']\n",
      "================================> This\n",
      "Eliminated as it is a stopword\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> confection\n",
      "Selected: Stem Word-> b'confect'\n",
      "================================> that\n",
      "Eliminated as it is a stopword\n",
      "================================> has\n",
      "Eliminated as it is a stopword\n",
      "================================> been\n",
      "Eliminated as it is a stopword\n",
      "================================> around\n",
      "Selected: Stem Word-> b'around'\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> few\n",
      "Eliminated as it is a stopword\n",
      "================================> centuries\n",
      "Selected: Stem Word-> b'centuri'\n",
      "================================> It\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> light\n",
      "Selected: Stem Word-> b'light'\n",
      "================================> pillowy\n",
      "Selected: Stem Word-> b'pillowi'\n",
      "================================> citrus\n",
      "Selected: Stem Word-> b'citrus'\n",
      "================================> gelatin\n",
      "Selected: Stem Word-> b'gelatin'\n",
      "================================> with\n",
      "Eliminated as it is a stopword\n",
      "================================> nuts\n",
      "Selected: Stem Word-> b'nut'\n",
      "================================> -\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> in\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> this\n",
      "Eliminated as it is a stopword\n",
      "================================> case\n",
      "Selected: Stem Word-> b'case'\n",
      "================================> Filberts\n",
      "Selected: Stem Word-> b'filbert'\n",
      "================================> And\n",
      "Eliminated as it is a stopword\n",
      "================================> it\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> cut\n",
      "Selected: Stem Word-> b'cut'\n",
      "================================> into\n",
      "Eliminated as it is a stopword\n",
      "================================> tiny\n",
      "Selected: Stem Word-> b'tini'\n",
      "================================> squares\n",
      "Selected: Stem Word-> b'squar'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> then\n",
      "Eliminated as it is a stopword\n",
      "================================> liberally\n",
      "Selected: Stem Word-> b'liber'\n",
      "================================> coated\n",
      "Selected: Stem Word-> b'coat'\n",
      "================================> with\n",
      "Eliminated as it is a stopword\n",
      "================================> powdered\n",
      "Selected: Stem Word-> b'powder'\n",
      "================================> sugar\n",
      "Selected: Stem Word-> b'sugar'\n",
      "================================> And\n",
      "Eliminated as it is a stopword\n",
      "================================> it\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> tiny\n",
      "Selected: Stem Word-> b'tini'\n",
      "================================> mouthful\n",
      "Selected: Stem Word-> b'mouth'\n",
      "================================> of\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> heaven\n",
      "Selected: Stem Word-> b'heaven'\n",
      "================================> Not\n",
      "Selected: Stem Word-> b'not'\n",
      "================================> too\n",
      "Eliminated as it is a stopword\n",
      "================================> chewy\n",
      "Selected: Stem Word-> b'chewi'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> very\n",
      "Eliminated as it is a stopword\n",
      "================================> flavorful\n",
      "Selected: Stem Word-> b'flavor'\n",
      "================================> I\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> highly\n",
      "Selected: Stem Word-> b'high'\n",
      "================================> recommend\n",
      "Selected: Stem Word-> b'recommend'\n",
      "================================> this\n",
      "Eliminated as it is a stopword\n",
      "================================> yummy\n",
      "Selected: Stem Word-> b'yummi'\n",
      "================================> treat\n",
      "Selected: Stem Word-> b'treat'\n",
      "================================> If\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> you\n",
      "Eliminated as it is a stopword\n",
      "================================> are\n",
      "Eliminated as it is a stopword\n",
      "================================> familiar\n",
      "Selected: Stem Word-> b'familiar'\n",
      "================================> with\n",
      "Eliminated as it is a stopword\n",
      "================================> the\n",
      "Eliminated as it is a stopword\n",
      "================================> story\n",
      "Selected: Stem Word-> b'stori'\n",
      "================================> of\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> CS\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> Lewis\n",
      "Selected: Stem Word-> b'lewi'\n",
      "================================> The\n",
      "Eliminated as it is a stopword\n",
      "================================> Lion\n",
      "Selected: Stem Word-> b'lion'\n",
      "================================> The\n",
      "Eliminated as it is a stopword\n",
      "================================> Witch\n",
      "Selected: Stem Word-> b'witch'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> The\n",
      "Eliminated as it is a stopword\n",
      "================================> Wardrobe\n",
      "Selected: Stem Word-> b'wardrob'\n",
      "================================> -\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> this\n",
      "Eliminated as it is a stopword\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> the\n",
      "Eliminated as it is a stopword\n",
      "================================> treat\n",
      "Selected: Stem Word-> b'treat'\n",
      "================================> that\n",
      "Eliminated as it is a stopword\n",
      "================================> seduces\n",
      "Selected: Stem Word-> b'seduc'\n",
      "================================> Edmund\n",
      "Selected: Stem Word-> b'edmund'\n",
      "================================> into\n",
      "Eliminated as it is a stopword\n",
      "================================> selling\n",
      "Selected: Stem Word-> b'sell'\n",
      "================================> out\n",
      "Eliminated as it is a stopword\n",
      "================================> his\n",
      "Eliminated as it is a stopword\n",
      "================================> Brother\n",
      "Selected: Stem Word-> b'brother'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> Sisters\n",
      "Selected: Stem Word-> b'sister'\n",
      "================================> to\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> the\n",
      "Eliminated as it is a stopword\n",
      "================================> Witch\n",
      "Selected: Stem Word-> b'witch'\n"
     ]
    }
   ],
   "source": [
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'][2:3].values: #Running only for 2nd review\n",
    "    filtered_sentence=[]\n",
    "    print(sent) #Each review\n",
    "    sent=striphtml(sent)# remove HTMl tags\n",
    "    sent=strippunc(sent)# remove Punctuation Symbols\n",
    "    print(sent.split())\n",
    "    for w in sent.split():\n",
    "        print(\"================================>\",w)\n",
    "        if((w.isalpha()) and (len(w)>2)):#If it is a numerical value or character of lenght less than 2    \n",
    "            if(w.lower() not in stop):# If it is a stopword\n",
    "                s=(snow.stem(w.lower())).encode('utf8') #Stemming the word using SnowBall Stemmer\n",
    "                print(\"Selected: Stem Word->\",s)\n",
    "                filtered_sentence.append(s)\n",
    "            else:\n",
    "                print(\"Eliminated as it is a stopword\")\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Eliminated as it is a numerical value or character of lenght less than 2\")\n",
    "            continue \n",
    "#     print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    #print(\"***********************************************************************\")\n",
    "    #print(\"Finally selected words from the review:\\n\",final_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# With stemming operation\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=striphtml(sent)# remove HTMl tags\n",
    "    sent=strippunc(sent)# remove Punctuation Symbols\n",
    "    for w in sent.split():\n",
    "        if((w.isalpha()) and (len(w)>2)):#If it is a numerical value or character of lenght less than 2    \n",
    "            if(w.lower() not in stop):# If it is a stopword\n",
    "                s=(snow.stem(w.lower())).encode('utf8') #Stemming the word using SnowBall Stemmer\n",
    "                filtered_sentence.append(s)\n",
    "                if (final['Score'].values)[i] == 'Positive': \n",
    "                    all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                if(final['Score'].values)[i] == 'Negative':\n",
    "                    all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "    #print(\"***********************************************************************\")\n",
    "    #print(\"Finally selected words from the review:\\n\",final_string)\n",
    "    i+=1\n",
    "print(len(final_string))\n",
    "final['CleanedText']=final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "Wall time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# without stemming operation\n",
    "i=0\n",
    "str1=' '\n",
    "nostem=[ ]\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=striphtml(sent)# remove HTMl tags\n",
    "    sent=strippunc(sent)# remove Punctuation Symbols\n",
    "    for w in sent.split():\n",
    "        if((w.isalpha()) and (len(w)>2)):#If it is a numerical value or character of lenght less than 2    \n",
    "            if(w.lower() not in stop):# If it is a stopword\n",
    "                s=w.lower().encode('utf8') #encoding as byte-string/utf-8\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue \n",
    "    str1 = b\" \".join(filtered_sentence)     \n",
    "   #print(str1)\n",
    "    nostem.append(str1)\n",
    "    #print(\"***********************************************************************\")\n",
    "    #print(\"Finally selected words from the review:\\n\",final_string)\n",
    "    i+=1\n",
    "print(len(nostem))\n",
    "final['CleanedText_NoStem']=nostem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>CleanedText_NoStem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>b'bought sever vital can dog food product foun...</td>\n",
       "      <td>b''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>b'product arriv label jumbo salt peanutsth pea...</td>\n",
       "      <td>b''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>b'confect around centuri light pillowi citrus ...</td>\n",
       "      <td>b''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>b'look secret ingredi robitussin believ found ...</td>\n",
       "      <td>b''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>b'great taffi great price wide assort yummi ta...</td>\n",
       "      <td>b''</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         CleanedText CleanedText_NoStem  \n",
       "0  b'bought sever vital can dog food product foun...                b''  \n",
       "1  b'product arriv label jumbo salt peanutsth pea...                b''  \n",
       "2  b'confect around centuri light pillowi citrus ...                b''  \n",
       "3  b'look secret ingredi robitussin believ found ...                b''  \n",
       "4  b'great taffi great price wide assort yummi ta...                b''  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5600,) (2400,) (5600,) (2400,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "##Sorting data according to Time in ascending order for Time Based Splitting\n",
    "time_sorted_data = final.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "x = time_sorted_data['CleanedText'].values\n",
    "y = time_sorted_data['Score']\n",
    "\n",
    "# split the data set into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer :  (5600, 1996)\n",
      "the number of unique words : 1996\n"
     ]
    }
   ],
   "source": [
    "#Bag of Words (BoW)\n",
    "count_vect = CountVectorizer(min_df = 10) \n",
    "X_train_vec = count_vect.fit_transform(X_train)\n",
    "X_test_vec = count_vect.transform(X_test)\n",
    "print(\"the type of count vectorizer :\",type(X_train_vec))\n",
    "print(\"the shape of out text BOW vectorizer : \",X_train_vec.get_shape())\n",
    "print(\"the number of unique words :\", X_train_vec.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train_vec_standardized = sc.fit_transform(X_train_vec)\n",
    "X_test_vec_standardized = sc.transform(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best parameters :\n",
      " SGDClassifier(alpha=1, average=False, class_weight=None, early_stopping=False,\n",
      "              epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "              learning_rate='optimal', loss='hinge', max_iter=1000,\n",
      "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
      "              random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Accuracy of the model :  0.6708333333333333\n",
      "The optimal value of alpha(1/C) is :  1\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV Implementation\n",
    "# Importing libraries\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Alpha = [0.0001,0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "param_grid = {'alpha': Alpha}\n",
    "model = GridSearchCV(SGDClassifier(), param_grid, scoring = 'f1_micro', cv=3 , n_jobs = -1,pre_dispatch=2)\n",
    "model.fit(X_train_vec_standardized, Y_train)\n",
    "print(\"Model with best parameters :\\n\",model.best_estimator_)\n",
    "print(\"Accuracy of the model : \",model.score(X_test_vec_standardized, Y_test))\n",
    "\n",
    "optimal_alpha = model.best_estimator_.alpha\n",
    "print(\"The optimal value of alpha(1/C) is : \",optimal_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Parameters\n",
      "Accuracy of the model at optimal hyperparameter alpha = 1.000000% is:  66.875000%\n",
      "f1 score value for   the model is: 0.5823405797447291\n",
      "precision score  for   the model is: 0.6015104575455846\n",
      "\n",
      "Train Parameters\n",
      "Accuracy of the model at optimal hyperparameter alpha = 1.000000% is:  72.267857%\n",
      "f1 score value for   the model is: 0.6590112835675291\n",
      "precision score  for   the model is: 0.7423481220764558\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "#confusion matrix,precision matrix,recall matrix,accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "sgd = SGDClassifier(alpha=optimal_alpha, n_jobs=-1)\n",
    "sgd.fit(X_train_vec_standardized,Y_train)\n",
    "print(\"Test Parameters\")\n",
    "Y_pred = sgd.predict(X_test_vec_standardized)\n",
    "Y_test_accuracy = accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)*100\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_test_accuracy))\n",
    "f1score= f1_score(Y_test, Y_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_test, Y_pred,average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)\n",
    "y_train_pred = sgd.predict(X_train_vec_standardized)\n",
    "print()\n",
    "Y_train_accuracy =accuracy_score(Y_train, y_train_pred, normalize=True, sample_weight=None)*100\n",
    "print(\"Train Parameters\")\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_train_accuracy))\n",
    "f1score= f1_score(Y_train, y_train_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_train, y_train_pred,pos_label='positive',average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best parameters :\n",
      " SGDClassifier(alpha=0.6237607226583327, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Accuracy of the model :  0.6741666666666667\n",
      "The optimal value of alpha(1/C) is :  0.6237607226583327\n",
      "\n",
      "Test Scores\n",
      "Accuracy of the model at optimal hyperparameter alpha = 0.623761% is:  67.375000%\n",
      "f1 score value for   the model is: 0.6012136484387484\n",
      "precision score  for   the model is: 0.5978721135014509\n",
      "\n",
      "Train Scores\n",
      "Accuracy of the model at optimal hyperparameter alpha = 0.623761% is:  76.107143%\n",
      "f1 score value for   the model is: 0.7203809898570251\n",
      "precision score  for   the model is: 0.7719504333397648\n"
     ]
    }
   ],
   "source": [
    "#Using Randomized Search CV to find best parameters\n",
    "# Load libraries\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "Alpha = uniform(loc=0, scale=1)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha=Alpha)\n",
    "\n",
    "#Using RandomizedSearchCV\n",
    "model = RandomizedSearchCV(SGDClassifier(), hyperparameters, scoring = 'f1_micro', cv=3 , n_jobs = -1,pre_dispatch=2)\n",
    "model.fit(X_train_vec_standardized, Y_train)\n",
    "print(\"Model with best parameters :\\n\",model.best_estimator_)\n",
    "print(\"Accuracy of the model : \",model.score(X_test_vec_standardized, Y_test))\n",
    "\n",
    "optimal_alpha = model.best_estimator_.alpha\n",
    "print(\"The optimal value of alpha(1/C) is : \",optimal_alpha)\n",
    "print()\n",
    "\n",
    "#precision matrix,recall matrix,accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "sgd = SGDClassifier(alpha=optimal_alpha, n_jobs=-1)\n",
    "sgd.fit(X_train_vec_standardized,Y_train)\n",
    "Y_pred = sgd.predict(X_test_vec_standardized)\n",
    "print(\"Test Scores\")\n",
    "\n",
    "Y_test_accuracy = accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)*100\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_test_accuracy))\n",
    "f1score= f1_score(Y_test, Y_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_test, Y_pred,average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)\n",
    "print()\n",
    "print(\"Train Scores\")\n",
    "y_train_pred = sgd.predict(X_train_vec_standardized)\n",
    "Y_train_accuracy =accuracy_score(Y_train, y_train_pred, normalize=True, sample_weight=None)*100\n",
    "#plot_confusion_matrix(Y_train, y_train_pred)\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_train_accuracy))\n",
    "f1score= f1_score(Y_train, y_train_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_train, y_train_pred,average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
